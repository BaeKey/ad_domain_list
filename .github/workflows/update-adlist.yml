name: Update AdBlock List

on:
  schedule:
    - cron: '0 20 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Process Domain Lists
        run: |
          python - <<EOF
          import requests
          import os
          import re

          # ================= é…ç½®åŒºåŸŸ =================
          AD_LIST_URL = "https://raw.githubusercontent.com/privacy-protection-tools/anti-AD/master/anti-ad-domains.txt"
          
          # è¿œç¨‹ç™½åå• (ç®€å•æ–‡æœ¬æ ¼å¼)
          REMOTE_WHITELISTS = [
              "https://raw.githubusercontent.com/privacy-protection-tools/anti-AD/refs/heads/master/discretion/dns.txt",
              "https://raw.githubusercontent.com/privacy-protection-tools/anti-AD/refs/heads/master/discretion/anv.txt"
          ]

          # anti-AD å®˜æ–¹ PHP ç™½åå•è§„åˆ™æ–‡ä»¶
          ANTI_AD_WL_PHP_URL = "https://raw.githubusercontent.com/privacy-protection-tools/anti-AD/refs/heads/adlist-maker/scripts/lib/white_domain_list.php"

          HAGEZI_ULTIMATE_URL = "https://raw.githubusercontent.com/hagezi/dns-blocklists/refs/heads/main/domains/ultimate.txt"

          LOCAL_WHITELIST_FILE = "whitelist.txt"
          LOCAL_EXTRA_DOMAINS_FILE = "extra_domains.txt"
          OUTPUT_FILE = "clean-list.txt"
          # ===========================================

          def download_text(url):
              try:
                  print(f"Downloading: {url}")
                  resp = requests.get(url, timeout=30)
                  resp.raise_for_status()
                  return resp.text
              except Exception as e:
                  print(f"Error downloading {url}: {e}")
                  return ""

          def parse_lines(text):
              lines = set()
              if not text: return lines
              for line in text.splitlines():
                  line = line.strip()
                  if not line or line.startswith("#"):
                      continue
                  lines.add(line)
              return lines

          def parse_php_rules(url):
              """
              ä» PHP æ–‡ä»¶ä¸­è§£æç™½åå•è§„åˆ™ã€‚
              PHP æ ¼å¼ç¤ºä¾‹: "domain.com" => 0,
              """
              print(f"æ­£åœ¨è·å–å¹¶è§£æ anti-AD PHP è§„åˆ™: {url}")
              text = download_text(url)
              if not text:
                  print("è­¦å‘Š: æ— æ³•è·å– PHP è§„åˆ™æ–‡ä»¶ï¼Œè·³è¿‡è§„åˆ™æ¸…æ´—ã€‚")
                  return {}

              rules = {}
              # æ­£åˆ™åŒ¹é…: å¼•å·ä¸­çš„åŸŸå => æ•°å­—
              # å…¼å®¹å•å¼•å·å’ŒåŒå¼•å·ï¼Œå¤„ç† => å‰åçš„ç©ºæ ¼
              pattern = re.compile(r"[\"']([\w\-\.]+)[\"']\s*=>\s*(-?\d+)")

              for line in text.splitlines():
                  line = line.strip()
                  # ç§»é™¤è¡Œå†…æ³¨é‡Š
                  if "//" in line:
                      line = line.split("//")[0].strip()
                  
                  # å¿½ç•¥ä»¥ # æˆ– // å¼€å¤´çš„è¡Œï¼ˆè™½ç„¶ä¸Šé¢å¤„ç†äº† //ï¼Œä½†é˜²æ­¢æ•´è¡Œæ³¨é‡Šï¼‰
                  if not line or line.startswith("#") or line.startswith("//"):
                      continue

                  match = pattern.search(line)
                  if match:
                      domain = match.group(1)
                      val = int(match.group(2))
                      rules[domain] = val
              
              print(f"æˆåŠŸè§£æå‡º {len(rules)} æ¡ anti-AD è§„åˆ™ã€‚")
              return rules

          def apply_anti_ad_rules(domains, rules):
              """
              æ ¹æ® anti-AD è§„åˆ™è¿›è¡Œè¿‡æ»¤ã€‚
              Value 0: ä»…ç§»é™¤è¯¥åŸŸå
              Value 1/2: ç§»é™¤è¯¥åŸŸååŠå…¶æ‰€æœ‰å­åŸŸå
              Value -1: å¿½ç•¥è§„åˆ™
              """
              if not rules:
                  return domains, 0

              initial_count = len(domains)
              
              # é¢„å¤„ç†è§„åˆ™åˆ†ç±»
              exact_whitelist = {d for d, v in rules.items() if v == 0}
              root_whitelist = {d for d, v in rules.items() if v in [1, 2]}
              
              # 1. ç²¾ç¡®åŒ¹é…è¿‡æ»¤ (Set å·®é›†è¿ç®—ï¼Œæå¿«)
              domains = domains - exact_whitelist
              
              # 2. å­åŸŸååŒ¹é…è¿‡æ»¤
              if root_whitelist:
                  filtered_domains = set()
                  for d in domains:
                      # æ£€æŸ¥æ˜¯å¦æ˜¯ç™½åå•æ ¹åŸŸåæœ¬èº«ï¼Œæˆ–è€…ä»¥ .root ç»“å°¾
                      should_remove = False
                      for root in root_whitelist:
                          if d == root or d.endswith("." + root):
                              should_remove = True
                              break
                      
                      if not should_remove:
                          filtered_domains.add(d)
                  domains = filtered_domains
                  
              removed_count = initial_count - len(domains)
              return domains, removed_count

          def main():
              # --- 1. anti-AD ä¸»åˆ—è¡¨ ---
              ad_text = download_text(AD_LIST_URL)
              ad_domains = parse_lines(ad_text)
              original_count = len(ad_domains)
              print(f"åŸå§‹ anti-AD æ•°é‡: {original_count}")

              # --- 2. æ–°å¢ï¼šä» Ultimate æå–ç‰¹å®šä¸»åŸŸåçš„å­åŸŸå ---
              added_from_ultimate = 0
              if os.path.exists(LOCAL_EXTRA_DOMAINS_FILE):
                  with open(LOCAL_EXTRA_DOMAINS_FILE, "r", encoding="utf-8") as f:
                      extra_main_domains = parse_lines(f.read())
                  
                  if extra_main_domains:
                      print(f"æ­£åœ¨ä» Ultimate åˆ—è¡¨æå– {len(extra_main_domains)} ä¸ªä¸»åŸŸåçš„å­åŸŸå...")
                      ultimate_text = download_text(HAGEZI_ULTIMATE_URL)
                      ultimate_domains = parse_lines(ultimate_text)
                      
                      extracted_subdomains = set()
                      for d in ultimate_domains:
                          if any(d == m or d.endswith("." + m) for m in extra_main_domains):
                              extracted_subdomains.add(d)
                      
                      before_add = len(ad_domains)
                      ad_domains.update(extracted_subdomains)
                      added_from_ultimate = len(ad_domains) - before_add
                      print(f"ä» Ultimate æå–å¹¶æ–°å¢äº† {added_from_ultimate} ä¸ªæ¡ç›®")

              count_before_whitelist = len(ad_domains)

              # --- 3. è¿œç¨‹ç™½åå•ï¼ˆæ–‡æœ¬åˆ—è¡¨ï¼Œç›´æ¥åˆ ï¼‰ ---
              remote_wl_domains = set()
              for url in REMOTE_WHITELISTS:
                  remote_wl_domains.update(parse_lines(download_text(url)))

              ad_domains -= remote_wl_domains
              removed_by_remote = count_before_whitelist - len(ad_domains)

              # --- 4. æœ¬åœ°ç™½åå•ï¼ˆtxtæ–‡ä»¶ï¼Œæ¨¡ç³ŠåŒ¹é…ï¼‰ ---
              user_rules = set()
              if os.path.exists(LOCAL_WHITELIST_FILE):
                  with open(LOCAL_WHITELIST_FILE, "r", encoding="utf-8") as f:
                      user_rules = parse_lines(f.read())

              removed_by_user = 0
              if user_rules:
                  count_before_local = len(ad_domains)
                  filtered = []
                  for domain in ad_domains:
                      if any(domain == r or domain.endswith("." + r) for r in user_rules):
                          continue
                      filtered.append(domain)
                  ad_domains = set(filtered)
                  removed_by_user = count_before_local - len(ad_domains)

              # --- 5. [Dynamic] Anti-AD è§„åˆ™æ¸…æ´— (æœ€åä¸€é“é˜²çº¿) ---
              # åŠ¨æ€ä¸‹è½½ PHP æ–‡ä»¶å¹¶è§£æ
              anti_ad_rules = parse_php_rules(ANTI_AD_WL_PHP_URL)
              ad_domains, removed_by_anti_ad_rules = apply_anti_ad_rules(ad_domains, anti_ad_rules)
              print(f"Anti-AD è§„åˆ™æ¸…æ´—ç§»é™¤äº† {removed_by_anti_ad_rules} ä¸ªæ¡ç›®")

              # --- 6. GitHub Summary ---
              final_count = len(ad_domains)
              summary = os.environ.get("GITHUB_STEP_SUMMARY")
              if summary:
                  with open(summary, "a", encoding="utf-8") as f:
                      f.write("### ğŸ›¡ï¸ å¹¿å‘Šè¿‡æ»¤ç»Ÿè®¡æŠ¥å‘Š (Anti-AD Core)\n")
                      f.write("| æ­¥éª¤ | å˜åŠ¨ | ç»“æœæ•°é‡ |\n")
                      f.write("| :--- | :--- | :--- |\n")
                      f.write(f"| 1. ğŸ“¥ åˆå§‹åŠ è½½ | **{original_count}** | {original_count} |\n")
                      f.write(f"| 2. â• Ultimate è¡¥å…… | +{added_from_ultimate} | {count_before_whitelist} |\n")
                      f.write(f"| 3. ğŸŒ è¿œç¨‹ç™½åå• | -{removed_by_remote} | {count_before_whitelist - removed_by_remote} |\n")
                      f.write(f"| 4. ğŸ‘¤ æœ¬åœ°ç™½åå• | -{removed_by_user} | {count_before_whitelist - removed_by_remote - removed_by_user} |\n")
                      f.write(f"| 5. ğŸ§¹ Anti-AD è§„åˆ™æ¸…æ´— | -{removed_by_anti_ad_rules} | **{final_count}** |\n")

              # --- 7. å†™å…¥æœ€ç»ˆæ–‡ä»¶ ---
              with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
                  for domain in sorted(ad_domains):
                      f.write(domain + "\n")

              print(f"å®Œæˆï¼Œè¾“å‡ºè‡³ {OUTPUT_FILE}")

          if __name__ == "__main__":
              main()
          EOF

      - name: Commit and push changes
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add clean-list.txt
          git diff --quiet && git diff --staged --quiet || (
            git commit -m "Update list: Dynamic anti-ad rules fetching" &&
            git push
          )
